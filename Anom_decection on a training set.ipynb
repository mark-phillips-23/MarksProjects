{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['AGN', 'CV', 'ILRT', 'LBV', 'LRN', 'Nova', 'QSO', 'SLSN-I',\n",
      "       'SLSN-II', 'SN', 'SN II', 'SN II-pec', 'SN IIb', 'SN IIn', 'SN Ia',\n",
      "       'SN Ia-91T', 'SN Ia-91bg-like', 'SN Ia-CSM', 'SN Ia-SC',\n",
      "       'SN Ia-pec', 'SN Iax', 'SN Ib', 'SN Ib-pec', 'SN Ib/c', 'SN Ibn',\n",
      "       'SN Ic', 'SN Ic-BL', 'SN Ic-Ca-rich', 'SN Ic-pec', 'SN Icn', 'TDE',\n",
      "       'Varstar'], dtype='<U17'), array([  47,  103,    1,    7,    1,   14,    4,   89,   49,   11,  957,\n",
      "          4,   58,  196, 4173,  157,   24,   15,    3,   32,   12,   82,\n",
      "          3,   14,   18,   92,   49,    1,    1,    2,   51,    8],\n",
      "      dtype=int64))\n",
      "['ZTF23aaetmwj' 'ZTF23aacjeou' 'ZTF23aacdnjz' ... 'ZTF23aaicagr'\n",
      " 'ZTF23aaikcdx' 'ZTF23aaitimz']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pmark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator ExtraTreeRegressor from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\pmark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator IsolationForest from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.9141448026491967 0.38613173510599885 -0.1323904130609125 0.447085940768306 1.3942834111714482 0.5258099231107676 1.2173300307554353 0.515399398137619 1.7293400913023902 0.5381560044030423 1.275369563097877 0.5198052312061633\n",
      "7.733317836371409 [0.39139649] ['SN Ia']\n",
      "[-2.16972373] 0.7895900863757621 ['SN Ia']\n",
      "['ZTF23aaetmwj']\n",
      "['ZTF23aaetmwj']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.cluster\n",
    "import math as m\n",
    "import sklearn.neighbors\n",
    "import sklearn.mixture\n",
    "import sklearn.ensemble\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Making a data set using the ZTF data\n",
    "# I am adding some conversions here for events which are either \"candidates\" or have <5 objects\n",
    "conversions = {'SN Ia-91T-like':'SN Ia-91T', 'SN IIn-pec':'SN IIn', 'TDE-H-He':'TDE', 'SN IIP':'SN II', \n",
    "               'SN IIL': 'SN II', 'SN Iax[02cx-like]': 'SN Iax'}\n",
    "def make_dataset(load=False):\n",
    "    # to do - eventually, I do want to save/load a dataset...\n",
    "    if not load:\n",
    "        metadata, all_labels = np.loadtxt('/Users/pmark/OneDrive/Desktop/Research Materials/Training set/training_set_dynesty.csv',usecols=(0,1), \n",
    "                                          delimiter=',', skiprows=1, dtype=str, unpack=True)\n",
    "        \n",
    "        gind = np.where((all_labels !='Galaxy') & (all_labels !='Other') &  (all_labels !='M dwarf') & (all_labels !='SN I'))\n",
    "        metadata = metadata[gind]\n",
    "        all_labels = all_labels[gind]\n",
    "        all_dat = np.zeros((len(all_labels),15))\n",
    "        names = np.array([])\n",
    "        for key in conversions:\n",
    "            gind = np.where(all_labels == key)\n",
    "            all_labels[gind] = conversions[key]\n",
    "        print(np.unique(all_labels, return_counts=True))\n",
    "\n",
    "        for i, md in enumerate(metadata):\n",
    "            one_sn_data = np.load('/Users/pmark/OneDrive/Desktop/Research Materials/Training set/dynesty_fits/'+md+'_eqwt_dynesty.npz')\n",
    "            all_dat[i] = np.mean(one_sn_data['arr_0'],axis=0)\n",
    "            names = np.append(str(md), names)\n",
    "        return(all_labels,all_dat, names)\n",
    "\n",
    "labels,all_features, names = make_dataset()\n",
    "print(names)\n",
    "all_features = np.log10(all_features[:, [2,4,5,6]])\n",
    "all_features = np.delete(all_features, [0,3], 1)\n",
    "\n",
    "# Loading in pre-existing GMM and IF files\n",
    "\n",
    "# Here I load in the existing mixture model and fit it to the new training set\n",
    "means = np.load(\"/Users/pmark/OneDrive/Desktop/Research Materials/Training set/Phillips_deSoto_gm_fit_means.npy\")\n",
    "weights = np.load('/Users/pmark/OneDrive/Desktop/Research Materials/Training set/Phillips_deSoto_gm_fit_weights.npy')\n",
    "covar = np.load('/Users/pmark/OneDrive/Desktop/Research Materials/Training set/Phillips_deSoto_gm_fit_covariances.npy')        \n",
    "loaded_gm = sklearn.mixture.GaussianMixture(n_components = len(means), random_state = 0, covariance_type = 'full')\n",
    "loaded_gm.means_ = means\n",
    "loaded_gm.covariances_ = covar\n",
    "loaded_gm.weights_ = weights\n",
    "loaded_gm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(covar))\n",
    "loaded_gmm = loaded_gm.fit(all_features)\n",
    "\n",
    "# Here I load the Isolation forest model and fit it to the new training set.\n",
    "IF_file = np.load('/Users/pmark/OneDrive/Desktop/Research Materials/Training set/Phillips_IsolationForest.npy', allow_pickle = True)\n",
    "loaded_IF = sklearn.ensemble.IsolationForest(n_estimators = len(IF_file))\n",
    "loaded_if = loaded_IF.fit(all_features)\n",
    "\n",
    "# Here I calculate the anomaly scores for the mixture model and isolation forest\n",
    "\n",
    "anom_scores_GMM = -1 * loaded_gmm.score_samples(all_features)\n",
    "anom_scores_IF = -1 * loaded_IF.score_samples(all_features)\n",
    "\n",
    "# GMM and IF anomaly scores by SN classification\n",
    "snia_anom_scoresgmm =np.array([]) \n",
    "snibc_anom_scoresgmm = np.array([]) \n",
    "snii_anom_scoresgmm = np.array([]) \n",
    "snii_bn_anom_scoresgmm =np.array([]) \n",
    "slsn_anom_scoresgmm = np.array([]) \n",
    "other_anom_scoresgmm = np.array([])\n",
    "\n",
    "snia_anom_scoresif =np.array([]) \n",
    "snibc_anom_scoresif = np.array([]) \n",
    "snii_anom_scoresif = np.array([]) \n",
    "snii_bn_anom_scoresif =np.array([]) \n",
    "slsn_anom_scoresif = np.array([]) \n",
    "other_anom_scoresif = np.array([])\n",
    "\n",
    "# Here I iteratively run through \n",
    "for i in range(len(labels)):\n",
    "    # Sorting all of the SNe that are type Ia\n",
    "    if str(labels[i]) == \"SN Ia\" or str(labels[i]) == \"SN Ia-91T\" or str(labels[i]) == \"SN Ia-91bg-like\"  or str(labels[i]) == \"SN Ia-CSM\" or str(labels[i]) == \"SN Ia-SC\" or str(labels[i]) == \"SN Ia-pec\" or str(labels[i]) == \"SN Iax\":\n",
    "        snia_anom_scoresgmm = np.append(anom_scores_GMM[i], snia_anom_scoresgmm)\n",
    "        snia_anom_scoresif = np.append(anom_scores_IF[i], snia_anom_scoresif)\n",
    "    # Sorting all of the type Ib/c SNe\n",
    "    elif str(labels[i]) == \"SN Ib\" or str(labels[i]) == \"SN Ic\" or str(labels[i]) == \"SN Ib/c\" or str(labels[i]) == \"SN Ibn\" or str(labels[i]) == \"SN Icn\" or str(labels[i]) == \"SN Ib-pec\" or str(labels[i]) == \"SN Ic-BL\" or str(labels[i]) == \"SN Ic-Ca-rich\" or str(labels[i]) == \"SN Ic-pec\":\n",
    "        snibc_anom_scoresgmm = np.append(anom_scores_GMM[i], snibc_anom_scoresgmm)\n",
    "        snibc_anom_scoresif = np.append(anom_scores_IF[i], snibc_anom_scoresif)\n",
    "    # Sorting all of the type II's that are not IIb or IIn's\n",
    "    elif str(labels[i]) == 'SN II' or str(labels[i]) == 'SN II-pec':\n",
    "        snii_anom_scoresgmm = np.append(anom_scores_GMM[i], snii_anom_scoresgmm)\n",
    "        snii_anom_scoresif = np.append(anom_scores_IF[i], snii_anom_scoresif)\n",
    "    # Sorting Type IIb's and IIn's\n",
    "    elif str(labels[i]) == 'SN IIb' or str(labels[i]) == 'SN IIn':\n",
    "        snii_bn_anom_scoresgmm = np.append(anom_scores_GMM[i], snii_bn_anom_scoresgmm)\n",
    "        snii_bn_anom_scoresif = np.append(anom_scores_IF[i], snii_bn_anom_scoresif)\n",
    "    # Sorting SLSN's\n",
    "    elif str(labels[i]) == 'SLSN-I' or str(labels[i]) == 'SLSN-II':\n",
    "        slsn_anom_scoresgmm = np.append(anom_scores_GMM[i], slsn_anom_scoresgmm)\n",
    "        slsn_anom_scoresif = np.append(anom_scores_IF[i], slsn_anom_scoresif)\n",
    "    # Sorting all other light curves\n",
    "    elif str(labels[i]) == 'AGN' or  str(labels[i]) == 'CV' or  str(labels[i]) == 'ILRT' or  str(labels[i]) == 'LBV' or  str(labels[i]) == 'LRN' or  str(labels[i]) == 'Nova' or  str(labels[i]) == 'QSO' or  str(labels[i]) == 'SN' or  str(labels[i]) == 'TDE' or  str(labels[i]) == 'Varstar':\n",
    "        other_anom_scoresgmm = np.append(anom_scores_GMM[i], other_anom_scoresgmm)\n",
    "        other_anom_scoresif = np.append(anom_scores_IF[i], other_anom_scoresif)\n",
    "        \n",
    "# Calculating the mean anomaly scores per population\n",
    "snia_meanscore_GMM = np.mean(snia_anom_scoresgmm)\n",
    "snia_meanscore_IF = np.mean(snia_anom_scoresif)\n",
    "\n",
    "snibc_meanscore_GMM = np.mean(snibc_anom_scoresgmm)\n",
    "snibc_meanscore_if = np.mean(snibc_anom_scoresif)\n",
    "\n",
    "snii_meanscore_GMM = np.mean(snii_anom_scoresgmm)\n",
    "snii_meanscore_IF = np.mean(snii_anom_scoresif)\n",
    "\n",
    "sniibn_meanscore_GMM = np.mean(snii_bn_anom_scoresgmm)\n",
    "sniibn_meanscore_IF = np.mean(snii_bn_anom_scoresif)\n",
    "\n",
    "slsn_meanscore_GMM = np.mean(slsn_anom_scoresgmm)\n",
    "slsn_meanscore_IF = np.mean(slsn_anom_scoresif)\n",
    "\n",
    "other_meanscore_GMM = np.mean(other_anom_scoresgmm)\n",
    "other_meanscore_IF = np.mean(other_anom_scoresif)\n",
    "\n",
    "print(snia_meanscore_GMM, snia_meanscore_IF, snibc_meanscore_GMM, snibc_meanscore_if, snii_meanscore_GMM, snii_meanscore_IF, sniibn_meanscore_GMM, sniibn_meanscore_IF, slsn_meanscore_GMM, slsn_meanscore_IF, other_meanscore_GMM, other_meanscore_IF)\n",
    "\n",
    "# Most Anomalous item in the GMM\n",
    "print(np.max(anom_scores_GMM), anom_scores_IF[np.where(np.max(anom_scores_GMM))[0]] , labels[np.where(np.max(anom_scores_GMM))[0]])\n",
    "\n",
    "# Most anomalous item in the IF\n",
    "print(anom_scores_GMM[np.where(np.max(anom_scores_IF))[0]], np.max(anom_scores_IF) , labels[np.where(np.max(anom_scores_IF))[0]])\n",
    "\n",
    "\n",
    "# Getting names of top anomaly for GMM using the features of the object\n",
    "anom_features_gmm = all_features[np.where(np.max(anom_scores_GMM))[0]]\n",
    "name_gmm_anom = names[np.where(np.max(anom_scores_GMM))[0]]\n",
    "print(name_gmm_anom)\n",
    "\n",
    "# Getting names of top anomaly for IF using the features of the object\n",
    "anom_features_if = all_features[np.where(np.max(anom_scores_IF))[0]]\n",
    "name_if_anom = names[np.where(np.max(anom_scores_IF))[0]]\n",
    "print(name_if_anom)\n",
    "\n",
    "\n",
    "# Finding the threshold scores for the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
